import pandas as pd
import re
import datetime
import glob

from tqdm import tqdm
# tokenize words with NLTK
from nltk import tokenize
from nltk.corpus import stopwords
from textblob import TextBlob
from joblib import Parallel, delayed

# paths to
# 1) text copy-pasted from pdf file, which includes dates #    pdf file generated by scrolling to first message and selecting print->pdf
# 2) text generated by scrolling up, then Ctrl+A followed by copy-paste
#    does not include dates, but has usernames

def convo2df(path):
    """
    Get conversation string into dataframe with metadata
    convo: str
        conversation, copy paste from messages app
    """
    with open(path) as f: convo = f.read()
    # get invididual texts, dates/hours, separated by linebreaks '\n'
    lines = convo.split('\n')
    text_df = pd.DataFrame()

    for l in tqdm(lines):

        try:
            # check if line is time and date
            time = re.search(r'(\d+:\d\d [A|P]M)', l).group(1)
            date = re.search(r'(\d+/\d+/\d+)', l).group(1)
            # convert to datetime format
            datetime = pd.to_datetime(date+' '+time)
            continue

        except AttributeError:

            # get author if available
            if 'Porfirio Cadena' in l:
                author='Porfi'
                continue
            elif phone_number in l:
                author='Ellen'
                continue
            # if not datetime or author, it's a text
            else: text = l.strip()

        # store in dataframe
        _df = pd.DataFrame()
        _df['text'] = [text]

        # add available metadata
        try: _df['author'] = author
        except NameError: pass

        try: _df['datetime'] = datetime
        except NameError: pass

        # store
        text_df = text_df.append(_df, ignore_index=True)

    return text_df

def token_text(text):
    """
    Tokenize and cleanup text
    """
    # split into words using NLTK's word tokenizer
    words = tokenize.word_tokenize(text)
    # remove punctuation signs and convert to lowercase
    words = [w.lower() for w in words if w.isalpha()]
    return words

# get conversation data
convo_date = Parallel(n_jobs=12)(delayed(convo2df)(p)
                for p in tqdm(glob.glob('../data/*frompdf.txt')))
convo_date = pd.concat(convo_date, ignore_index=False)
# authorship is not relevant here
convo_date = convo_date.drop('author', axis=1)

convo_txt = Parallel(n_jobs=12)(delayed(convo2df)(p)
                for p in tqdm(glob.glob('../data/*txt')) if 'pdf' not in p)
convo_txt = pd.concat(convo_txt, ignore_index=False)

convo_df = pd.merge(convo_txt, convo_date, on='text')
# drop duplicates that originate from merging info from pdf and with old dfs
convo_df = convo_df.drop_duplicates(['text','datetime','author'])

# tokenize texts
convo_df['words'] = convo_df.text.apply(token_text)
convo_df[['polarity','subject']] = convo_df.words.apply(lambda x:\
                                    pd.Series(TextBlob(' '.join(x)).sentiment))

convo_df.to_csv('../output/convo_01022020.csv', index=False)



convo_df.words.apply(lambda x: pd.Series(TextBlob(x).sentiment))
summ = convo_df.groupby('author').resample('D', on='datetime').median()
plt.plot(summ.loc['Porfi',:].polarity.values, label='Porfi')
plt.plot(summ.loc['Ellen',:].polarity.values)

# Explore in notebook
# distribution of text lengths, comment on exponential wo mentioning name
# distribution of unique words by text, Ellen is native speaker, bigger
# vocabulary
# analyze sentiment by day not by text
# sentiment score over time
# example texts of good and bad sentiment
convo_df[convo_df.text.str.contains(r'love you|Love you')].groupby(\
        'author').count()#.sort_values(by='datetime')[['datetime', 'text','author']]

convo_df[convo_df.text.str.contains(r'nice')]

# size of vocabulary?
word_set = convo_df.groupby('author').words.sum().apply(set)
# by user
word_set.apply(len)
# words I say but not Ellen make wordcloud of this!
porfi = word_set['Porfi'].difference(word_set['Ellen'])
# ellens vocabulary
ellen = word_set['Ellen'].difference(word_set['Porfi'])
wordcloud = WordCloud(max_font_size=50, max_words=100, background_color="white").generate(' '.join(porfi))
# Display the generated image:
plt.figure()
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")

# make tidy word dataframe
def tokenize_row(row, remove_stop=True):
    """
    Tokenize and cleanup text
    """
    text = row.text
    # split into words using NLTK's word tokenizer
    words = tokenize.word_tokenize(text)
    # remove punctuation signs and convert to lowercase
    words = [w.lower() for w in words if w.isalpha()]
    if remove_stop:
        # remove stopwords
        stops = set(stopwords.words("english"))
        words = [w for w in words if w not in stops]
    # store in dataframe
    df = pd.DataFrame(words, columns=['word'])
    df['author'] = row.author
    df['datetime'] = row.datetime
    df['text'] = text
    return df

convo_token = Parallel(n_jobs=12)(delayed(tokenize_row)(row)
                        for _, row in tqdm(convo_df.iterrows()))
# unpack text dataframes
convo_token = pd.concat(convo_token, ignore_index=False)
word_counts = convo_token.groupby('author').word.value_counts()
word_counts.name = 'counts'
word_counts = word_counts.reset_index()
plot_ecdf(word_counts.counts.values)
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
# lower max_font_size, change the maximum number of word and lighten the background:
wordcloud = WordCloud(max_font_size=50, max_words=100, background_color="white").generate(' '.join(convo_token[convo_token.author=='Porfi'].word.values))
# Display the generated image:
plt.figure()
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")

# lower max_font_size, change the maximum number of word and lighten the background:
wordcloud = WordCloud(max_font_size=50, max_words=100, background_color="white").generate(' '.join(convo_token[convo_token.author=='Ellen'].word.values))
# Display the generated image:
plt.figure()
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")

wordcloud = WordCloud(max_font_size=50, max_words=100, background_color="white").generate(job2)
# Display the generated image:
plt.figure()
plt.imshow(wordcloud, interpolation='bilinear')
# filter by date
from datetime import date
from datetime import date
convo_df[(convo_df.datetime>pd.Timestamp(date(2019,1,1)))&(convo_df.polarity<0)].sort_values('datetime').tail(10)
